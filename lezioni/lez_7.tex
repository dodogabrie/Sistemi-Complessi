\section{Lezione 7}%
\label{sub:Lezione 7}

\subsection{Integrali stocastici}%
\label{sub:Integrali stocastici}
Sia $x$ una variabile stocastica, il differenziale di questa variabile lo definiamo come:
\begin{equation}
    dx = d\omega (t) \label{eq:6_int}
\end{equation}
Ipotizziamo che il processo stocastico sia un processo di Wiener, in tal caso:
\[
    P(d\omega) \sim \exp\left(-\frac{\left(d\omega\right)^2}{dt}\right)
.\] 
Con $dt$ differenziale temporale.\\
Prendiamo allora una funzione $G(s)$, vogliamo definire cosa significa calcolare l'integrale di $G(s)$ se la misura è stocastica.
\begin{figure}[H]
    \centering
    \incfig{lez_6_int}
    \caption{\scriptsize Funzione $G(t)$ con punti stocastici $\omega_i$, $\Delta\omega_i$ è la distanza sull'asse $y$ tra il punto $\omega_{i-1}$ e $\omega_i$.}
    \label{fig:lez_6_int}
\end{figure}
Definiamo allora l'integrale come:
\begin{redbox}{Integrale stocastico}
\[
    \int_{t_0}^{t_n} G(s) d\omega (s) \equiv \lim^{\text{ms}}_{n \to \infty} \sum_{i}^{} G(\tau_i) \left[\omega (t_i) - \omega (t_{i-1}) \right]
\]     
Il valore dell'integrale dipende dalla scelta dei $\tau_i$.
\end{redbox}
\noindent
\'E interessante utilizzare come $G(t)$ il processo di Wiener stesso per vedere cosa succede:
\[
    G(t) = \omega (t) 
.\]
Inoltre definiamo gli step $\tau_i$ come:
\[\begin{aligned}
    \tau_i = t_{i-1} + \alpha (t_i - t_{i-1}) \qquad 0 <\alpha <1 \label{eq:tau}
.\end{aligned}\]
Valutiamo la sommatoria all'interno della definizione:
\[\begin{aligned}
    \left<S_n\right> =& \sum_{i=0}^{n} \left<\omega (\tau_i)\left[ \omega (t_i) - \omega (t_{i-1})  \right] \right>=\\
		      & = \sum_{i=0}^{n} \left<\omega (t_{i-1} +\alpha (t_i-t_{i-1}) ) \omega(t_i) \right> + \\
		  & \qquad \qquad - \left<\omega (t_{i-1} +\alpha (t_i-t_{i-1}) ) \omega(t_{i-1}) \right>
.\end{aligned}\]
Ricordando che nei processi di Wiener vale:
\[
    \left<\omega (t) \omega (s) \right> = \text{min}(s,t) 
.\] 
Rimane soltanto:
\[\begin{aligned}
    \left<S_n\right> =& \sum_{i=0}^{n} t_{i-1} + \alpha (t_i-t_{i-1}) -\sum_{i=0}^{n} t_{i-1} = \\
		      & =\alpha(t_n-t_0) 
.\end{aligned}\]
Di conseguenza con la scelta \ref{eq:tau} per i $\tau_i$ contano solo l'istante finale ed iniziale. \\
Inoltre quando $\alpha =0$ l'integrale si annulla, mentre quando $\alpha =1$ l'integrale è l'intervallo temporale.\\
La vera domanda da porsi è quale sia il giusto valore di $\alpha$\ldots
\subsection{Integrale di $\hat{\text{I}}$to e di Stratonovich}%
\label{sub:Integrale di Ito e di Stratonovich}
\subsubsection{Integrale di $\hat{\text{I}}$to}%
\label{subsub:Integrale di Ito}
$\hat{\text{I}}$to è un matematico Giapponese, integrare con $\hat{\text{I}}$to implica scegliere $\tau_i$ all'inizio dell'intervallo.
\begin{bluebox}{Integrale di $\hat{\text{I}}$to}
    \[
        \alpha  = 0
    .\] 
    \[
	\tau_i = t_{i-1}
    .\] 
\end{bluebox}
\noindent
Le somme parziali con questo integrale si scrivono come:
\[
    S_n = \sum_{i}^{} \omega (t_{i-1}) \left[\omega (t_i) -\omega (t_{i-1}) \right]
.\]
L'integrazione di $\hat{\text{I}}$to forma una \texttt{Martingala}.
\paragraph{Martingala}%
\label{par:Martingala}
Dato un set di variabili stocastiche:
\[
    \left\{x_i\right\}: E(\left|x_i\right|) < \infty
.\] 
\[
    \left\{x_i\right\} \text{ è marting.} \iff
    E(x_{n+1}|x_1,\ldots,x_n) = x_n
.\] 
Con $E$: valore di aspettazione.\\
Possiamo notare che il processo di Wiener realizza una martingala perché rispetta questa proprietà.\\
Il calcolo di $\hat{\text{I}}$to è anche non anticipante:
\begin{redbox}{Funzione non anticipante}
    $G(t)$ è non anticipante se è indipendente dall'incremento $\omega (t) - \omega (s) $ $\forall t, s$.
\end{redbox}
\noindent
\begin{exmp}[Esempi di funzioni non anticipanti]
    Dato un processo di Wiener $\omega (t)$ tutte le seguenti funzioni sono non anticipanti:
    \begin{itemize}
	\item $\omega (t) $.
	\item $\int dt f(\omega (t) ) $.
	\item $\int d\omega f(\omega (t) ) $.
    \end{itemize}
\end{exmp}
\noindent
\subsubsection{Integrale di Stratonovich}%
\label{subsub:Integrale di Stratonovich}
Stratonovich era un fisico russo, integrare con Stratonovich implica scegliere il centro dell'intervallo.
\begin{bluebox}{Integrale di Stratonovich}
    \[
        \alpha = \frac{1}{2}
    .\] 
    \[
        \tau_i = \frac{1}{2}\left(\tau_{i-1}+ \tau_i\right)
    .\] 
\end{bluebox}
\noindent
Le somme parziali in questo caso si scrivono come:
\[
    S_n = \sum_{i}^{} \omega\left(\frac{t_i + t_{i-1}}{2}\right)\left[\omega (t_i) -\omega (t_{i-1}) \right]
.\] 
L'integrale di Stratonovich ha caratteristiche analoghe a quello che si usa normalmente in fisica, infatti si applica bene con funzioni "morbide".

\begin{exmp}[]
    \[
	\int_{0}^{t} \omega (t) dt = 
	\begin{cases}
	    \dfrac{\omega^2(t)}{2}-\dfrac{\omega^2(0)}{2} = \dfrac{t}{2} & \text{ Strato}\\
	    \sum_{}^{} \omega_{i-1}\left(\omega_i-\omega_{i-1}\right) = 0 & \text{ }\hat{\text{I}}\text{to}
	\end{cases}
    \] 
\end{exmp}
\noindent

\subsection{Relazione tra l'incremento stocastico e l'incremento temporale.}%
\label{sub:Relazione tra l'incremento stocastico e l'incremento temporale.}
La relazione tra i due differenziali è la seguente:
\begin{greenbox}{}
\[
    \left(d\omega\right)^2\sim dt
.\] 
\end{greenbox}
\noindent
Questo significa che $d\omega$  è continuo ma non è differenziabile. Abbiamo già accennato alla non differenziabilità dei processi di Wiener, ecco un'altra riprova.\\
Tutti gli ordini più alti dell'incremento si annullano:
\[
    d\omega^{N+2} \sim 0 \qquad \forall N>0
.\] 
Più formalmente, consideriamo il seguente integrale:
\[\begin{aligned}
    \int\left(d\omega\right)^{2+N}G(t) = \lim^{\text{ms}}_{n \to \infty} \sum_{i=0}^{n} G_{i-1} (\Delta\omega_i)^{2+N} 
.\end{aligned}\]
Quello che si può dimostrare è che:
\[
  \lim^{\text{ms}}_{n \to \infty} \sum_{i=0}^{n} G_{i-1} (\Delta\omega_i)^{2+N} = 
  \begin{cases}
      \int dtG(t) & N=0\\
      0          & N>0 
  \end{cases}
\] 
Quindi anche che:
\[
    \int\left(d\omega\right)^2 G(t) = \int dtG(t) 
.\] 
\begin{redbox}{}
    \begin{equation}
	d\omega  \sim O(dt^{1 /2}) \label{eq:6_order}
    \end{equation}
\end{redbox}
\noindent
\subsubsection{Applicazione: Differenziale di una funzione}%
\label{subsub:Applicazione: Differenziale di una funzione}
Prendiamo una funzione del tempo e del processo di Wiener: $f\left[\omega,t\right]$. Visto che si ha la \ref{eq:6_order} il differenziale $df$  all'ordine più basso è:
\begin{bluebox}{Differenziale di una funzione}
    \[
	df\left[\omega,t\right] = \left[\frac{\partial d}{\partial t} + \frac{1}{2}\frac{\partial ^2 f}{\partial \omega^2} \right]dt + \frac{\partial f}{\partial \omega} d\omega
    .\] 
\end{bluebox}
\noindent
Questa struttura per il differenziale di una funzione è profondamente legata alla formula di $\hat{\text{I}}$to.
\subsection{Formula di $\hat{\text{I}}$to}%
\label{sub:Formula di ITO}
Supponiamo di avere una SDE della seguente forma:
\[
    dx = a(x,t) dt + b(x,t) d\omega
.\] 
La soluzione formale è del seguente tipo:
\[
    x(t) = x_0 + \int_{t_0}^{t} a(x,s) ds + \int_{t_0}^{t} b(x,s) d\omega (s)   
.\] 
Supponiamo che esista una ed una sola soluzione non anticipante
\footnote{Le ipotesi per cui vale sono negli appunti}. \\
Allora se ho una $f(x,t)$ con $x$ soluzione della SDE scrivendone il differenziale all'ordine più basso si ha:
\begin{redbox}{Formula di $\hat{\text{I}}$to}
    \[\begin{aligned}
	df(x,t) = 
	\left[\frac{\partial f}{\partial t} + 
	a \frac{\partial f}{\partial x} +
        \frac{1}{2}b^2 \frac{\partial ^2 f}{\partial x^2} \right] dt +
	b\frac{\partial f}{\partial x} d\omega 
    .\end{aligned}\]
    con $dx = adt + bd\omega$.
\end{redbox}
\noindent
L'utilità della formula è che ci permette di fare cambi di variabili con funzioni dipendenti da una variabile casuale.

\subsection{Integrale di una SDE}%
\label{sub:Integrale di una SDE}
Prendiamo una SDE (Stochastical Differential Equation) del seguente tipo:
\[
    dx = f(x) dt + g(x) d\omega
.\] 
Con $\omega$ processo di Wiener. \\
Nell'equazione abbiamo una parte deterministica ($f(x) dt$) ed una stocastica ($g(x)d\omega$).
\begin{figure}[H]
    \centering
    \incfig{lez_7_int}
    \caption{\scriptsize La linea rappresenta l'incremento della parte deterministica, in alto abbiamo invece il processo stocastico che discosta la $x$ dalla parte di funzione deterministica (come un rumore sovrapposto al segnale).}
    \label{fig:lez_7_int}
\end{figure}
\noindent
Abbiamo detto che formalmente possiamo integrare nel seguente modo (con $h$  passo di integrazione):
\[
    x_h - x_0 = \int_{0}^{h}  f(x(s) ) ds + \int_{0}^{h} g(x(s)) d\omega
.\] 
La formalità dell'espressione deriva dal fatto che le funzioni $f$ e $g$ dipendono da $x$, quindi non possiamo semplicemente risolvere questo integrale.
\subsubsection{Soluzione perturbativa}%
\label{subsub:Soluzione perturbativa}
Se prendiamo un passo di integrazione $h$ piccolo, possiamo sviluppare $f$ e $g$ attorno al punto $x_0$:
\[\begin{aligned}
    f(x_s)   &=f_0 + f'_0\delta x_s + \frac{1}{2}f''_0(\delta x_s) ^2 \\
    g(x_s)   &= g_0 + g'_0\delta x_s + \frac{1}{2}g''_0(\delta x_s) ^2 
.\end{aligned}\]
Con $\delta x_s = x_s-x_0$. Sostituendo nella equazione per la soluzione formale e tenendo solo l'ordine più basso si ha:
\[
    \delta x_h = \int_{0}^{h} f_0ds + \int_{0}^{h} g_0d\omega  = f_0h + g_0  \int_{0}^{h} d\omega 
.\] 
Al secondo termine abbiamo un integrale stocastico. Questo indica che, operativamente, per effettuare una integrazione numerica e calcolare il punto successivo $x_{n+1}$ si deve:
\begin{itemize}
    \item Valutare la $f$ nel punto $x_n$.
    \item Valutare la $g$ nel punto $x_n$ (la $g$ di per se è solo una funzione, se la variabile è deterministica anche la $g$ da un risultato deterministico).
    \item Ipotizzare una distribuzione per $\omega$.
    \item Estrarre ogni volta un valore $Z_n$ secondo tale distribuzione
    \footnote{caratterizzeremo meglio tale distribuzione sotto}
     facendo in modo che, alla fine del processo, i valori siano distribuiti secondo la distribuzione di $\omega$.
    \item $x_{n+1} = f(x_n)h + g(x_n) Z_n$ 
\end{itemize}
Il procedimento funziona perché l'integrale:
\[
    \int_{0}^{h} d\omega 
.\] 
\'E la somma di variabili Gaussiane, di conseguenza è anch'esso un processo con distribuzione Gaussiana:
\[
    Z_1(h) \equiv \int_{0}^{h} d\omega 
.\]
Vediamo le proprietà di $Z_1$:
\[
    \left<Z_1(h)\right> = \int_{0}^{h} \left<d\omega\right> = 0
.\] 
Poiché il processo di Wiener ha media nulla.
\[\begin{aligned}
    \left<Z^2(h) \right> = &\left<\int_{0}^{h} d\omega_s \int_{0}^{h} d\omega_t \right> = \\
			   &=\left<\sum_{i}^{} \left(\omega_i-\omega_{i-1}\right) \sum_{J}^{} \left(\omega_J-\omega_{J-1}\right)\right> = \\
			   &= \sum_{i}^{} \sum_{J}^{} \Delta t\delta_{iJ} = h
.\end{aligned}\]
Dove per risolvere si è usato che:
\[
    \left<\Delta\omega^2\right> = \Delta t
.\] 
Se ne conclude che la variabile $Z_1$ è una Gaussiana a media nulla e con varianza $\sqrt{h}$:
\[
    Z_1 \in G(0,\sqrt{h}) 
.\] 
Operativamente possiamo generare un numero random tra $0$ e $1$:
\[
    Y_1(i) \in G(0,1) 
.\] 
Ed ottenere la variabile da moltiplicare a $g_0$ con:
\[
    Z_1(h) = \sqrt{h} Y_1(i) 
.\] 
In conclusione si ha che:
\begin{redbox}{}
\[
    \delta x_h = f_0 h + g_0Z_1(h) 
.\]     
\end{redbox}
\noindent
Guardando l'espressione notiamo che il primo termine è di ordine $h$ mentre il secondo è di ordine $\sqrt{h}$  poiché è un processo di Wiener. \\
Risulta quindi necessario capire se ci siamo persi dei termini di ordine $h$  nella parte di sviluppo stocastico.\\
Possiamo prendere la soluzione perturbativa al primo ordine e inserirla nuovamente all'interno dello sviluppo.\\
Ci limitiamo inoltre ad inserire solo il termine all'ordine più basso ($g_0Z_1(h)$) poiché il termine con $f_0$  darebbe sicuramente contributi di ordine superiore.
\[
    \delta x_s^{(1 /2)} = g_0Z_1(h) = g_0  \int_{0}^{h} d\omega 
.\] 
\[\begin{aligned}
    \delta x_t = &\int_{0}^{t} \left(f_0 + f'_0 g_0\int_{0}^{s}d\omega_r  \right) ds + \\
		 & + \int_{0}^{h} \left(g_0+ g'_0 g_0 \int_{0}^{s} d\omega_r\right)d\omega_s  
.\end{aligned}\]
L'unico contributo di ordine $h$  deriva dal secondo integrale, che ci da un termine del tipo:
\[
    \int_{0}^{t} \int_{0}^{s} d\omega_r d\omega_s = \int_{0}^{t} \omega_sd\omega_s 
.\] 
Quindi adesso dobbiamo decidere quale calcolo utilizzare: $\hat{\text{I}}$to oppure Stratonovich. 
\begin{bluebox}{}
    L'evoluzione dell'equazione differenziale stocastica dipende dalla scelta del metodo di integrazione.
\end{bluebox}
\[
  \int_{0}^{t} \omega_sd\omega_s =
  \begin{cases}
      \dfrac{\omega_t^2}{2} & \text{Strato}\\
      \dfrac{1}{2}\left(\dfrac{\omega_t^2}{2} - t\right) & \hat{\text{I}}\text{to}
  \end{cases}
\] 
In entrambi i casi si ottiene un termine $O(h)$, quindi:
\begin{redbox}{}
 \[
    \delta x_h = 
    g_0Z_1(h) +
    f_0h + 
    \frac{g_0g'_0}{2}\cdot \alpha (\hat{\text{I}}, S) 
\] 
Con $\alpha (\hat{\text{I}}, S)$ data da:
\[
    \alpha (\hat{\text{I}}, S) = 
    \begin{cases}
	Z_1^2(h) & \text{Strato}\\
	Z_1^2(h) - h & \hat{\text{I}}to
    \end{cases}
\]    
\end{redbox}
\noindent
\subsubsection{Uguaglianza tra i due metodi}%
\label{subsub:Uguaglianza tra i due metodi}
Effettuando il seguente cambio di variabili:
\[
    dx = \left(f-\frac{1}{2}gg'\right)dt + gd\omega
.\] 
si ha che i due $\delta x_h$ ($\hat{\text{I}}$to e Stratonovich) si eguagliano poiché il termine aggiunto va a compensare il termine che subentra con l'integrale di $\hat{\text{I}}$to.\\
L'importanza di questo "cambio di variabili" è che ci autorizza ad utilizzare l'approccio di Stratonovich anche per sistemi che fisicamente andrebbero trattati con $\hat{\text{I}}$to
\footnote{Stratonovich permette algoritmi di integrazione più potenti.}.
\subsection{Algoritmo di Heun}%
\label{sub:Algoritmo di Heun}
Operativamente (per davvero) si usa spesso l'algoritmo di Heun per l'integrazione di SDE: si tratta di un algoritmo a 3 step:
\[\begin{aligned}
    & \tilde{x}_1 = x_0 + Z_1g_0 + f_0 h  + \frac{1}{2}g_0g'_0 Z_1^2\\
    & x_1 = x_0 + Z_1 g(\tilde{x}_0) + f(\tilde{x}_0)  + \frac{1}{2}g(\tilde{x}_0) g'(\tilde{x}_0) Z_1^2\\
    & x_h = \frac{1}{2}\left(x_1+ \tilde{x}_1\right)
.\end{aligned}\]
Sostanzialmente equivale a fare un primo step di predizione ed un successivo step di correzione.
\clearpage
