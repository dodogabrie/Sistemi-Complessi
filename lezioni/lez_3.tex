\section{Funzione caratteristica e Momenti Fattoriali}%
\label{sub:Lezione 3}
\mylocaltoc
\subsection{Funzione caratteristica e cumulanti}%
\label{sub:Funzione caratteristica}
Sia $\vect{x}$ una variabile random con distribuzione di probabilità $P(\vect{x})$, la funzione caratteristica della distribuzione è la sua trasformata di Fourier:
\begin{redbox}{}
    \[
	\phi (\vect{s}) = \int d\vect{x} P(\vect{x}) e^{i \vect{x}\cdot \vect{s}}
    .\] 
\end{redbox}

\begin{exmp}[Distribuzione Gaussiana]
 \[\begin{aligned}
    &P(x) = \frac{e^{-x^2 / 2 \sigma^2}}{\sqrt{2\pi\sigma^2}} 
    &\implies&
    &\phi (s) = e^{-\sigma^2s^2 / 2}
.\end{aligned}\]
\end{exmp}

\begin{exmp}[Distribuzione uniforme]
\[
    P(x) = 
    \begin{cases}
	1 & x \in \left[-\frac{1}{2}, \frac{1}{2}\right]\\
	0 & \text{Altrimenti}
    \end{cases}
\] 
\[
    \phi (s) = \int_{-1 /2}^{1 /2} e^{ixs}dx = \frac{2}{s}\sin\left(\frac{s}{2}\right)  
.\] 
\end{exmp}

\subsubsection{Proprietà della funzione caratteristica}%
\label{ssub:Proprietà della funzione caratteristica}
\begin{enumerate}
    \item $\left|\phi (0) \right|= 1 $.
    \item $\phi (s) $ è continua.
    \item Se $\exists \left<x^n\right>$ allora: 
	\[
		\left<x^n\right> = \left(-i\right)^n \left.\frac{\partial ^n}{\partial s^n} \phi (s)\right|_{s=0} 
	.\] 
    \item Una sequenza di distribuzioni converge ad una distribuzione limite $\iff$ converge la sequenza di funzioni caratteristiche.
    \item Dato $\vect{x} = \left(x_1, \ldots, x_n\right)$ con $x_i$ indipendenti $\forall i$ allora:
	\[ 
	    \phi (s_1, s_2, \ldots) = \prod_{i=1}^{n} \phi (s_i)  
	\]
    \item Se $y = \sum_{i}^{} x_i$ con $x_i$ indipendenti, allora:
	\[
	    \phi (s) = \left<e^{isy}\right> = \prod_{i}^{} \phi_i(s)  
	.\] 
	\begin{exmp}
	    Se $y = x_1 + x_2$ ho che:
	    \[
		P(y) = \int P(x_1) P(y-x_1) dx_1
	    .\] 
	    Allora per le proprietà della trasformata di una convoluzione:
	    \[
		\phi (s) = \phi_1(s) \phi_2(s) 
	    .\] 
	\end{exmp}
\end{enumerate}
\begin{exmp}[Testa o Croce]
    Prendiamo una distribuzione che corrisponda alla probabilità del set di eventi ["testa","croce"] dopo il lancio di una moneta:
    \[
        P(x) = \frac{1}{2}\left( \delta(x-1) + \delta(x+1) \right) 
    \]
    Calcoliamo la funzione caratteristica di tale distribuzione:
    \[
	\phi(s) = \int P(x) e^{isx} dx = \frac{1}{2}\left[ e^{is} + e^{-is} \right] 
    \]
    Ipotizziamo di fare $n$ lanci di moneta e di voler inferire tramite la funzione caratteristica quale sarà la distribuzione di probabilità finale. \\
    Stiamo parlando della probabilità di una somma di eventi indipendenti, quindi per quanto visto in precedenza si ha che:
    \[
	\phi_n(s) = \left[ \phi(s) \right] ^n = \frac{1}{2^n} \left( e^{is} + e^{-is} \right)^n 
    \]
    Si sfrutta la formula binomiale (somma di elementi elevati alla $n$):
    \[
	\phi_n(s) = \frac{1}{2^n}\sum_{k = 0}^{n}\binom{n}{k} e^{is(n-2k)}
    \]
    La trasformata inversa assume la forma di una somma di delta di Dirac che dovrebbero convergere ad una Gaussiana nel limite di $n\to\infty$:
    \[
	P(x) = \frac{1}{2\pi}\int ds \phi_n(s) e^{-ixs} = \frac{1}{2^n} \sum_{k = 0}^{n} \binom{n}{k}\delta(n-2k)
    \]
\end{exmp}
\subsubsection{Cumulanti della funzione caratteristica.}%
\label{sub:Sviluppo in cumulanti di phi}
\begin{redbox}{Funzione generatrice dei cumulanti}
   \[
       \Phi(s) = \ln (\phi (s) ) 
   .\]  
\end{redbox}
\noindent
Si potrebbe dimostrare che la funzione generatrice si esprime in modo generale in funzione di quantità definite come cumulanti:
\[\begin{aligned}
    \Phi = \sum_{r=1}^{\infty} i^r \sum_{\left\{m\right\}}^{} 
    \left<\left< x_1^{m_1}x_2^{m_2}\ldots\right>\right> 
    \prod_{i=1}^{\infty} \frac{s_i^{m_i}}{m_i!\,}\delta (r-\sum_{i=1}^{r} m_r) 
.\end{aligned}\]
Dove i termini tra le parentesi $\left<\left< x_i^{m_i}\right>\right>$ sono i cumulanti. Prendiamo ad esempio lo sviluppo dei primi due:
\[\begin{aligned}
    & \left<\left<x_1\right>\right> = \left<x_1\right> \sim \text{Media}\\
    & \left<\left<x_1 x_2 \right>\right> = \left<x_1x_2\right> - \left<x_1\right>\left<x_2\right> \sim \text{ Covarianza}
.\end{aligned}\]
Consideriamo adesso i cumulanti per una stessa variabile stocastica ($x_i = x_j $ $\forall i, j$), che chiameremo in questo contesto anche \texttt{Momenti}.
\begin{greenbox}{Cumulanti di processo Gaussiano.}
   I cumulanti per un processo Gaussiano sono tutti nulli per $n\ge 3$.
   \[
       \left<\left<x^n\right>\right> = 0 \quad \forall n \ge  3
   .\] 
\end{greenbox}
\begin{exmp}[Cumulante quarto per Gaussiana]
    \[\begin{aligned}
	\left<\left<x^4\right>\right> =& \left<x^4\right>-4\left<x^3\right>\left<x\right>+\\
					&-3\left<x^2\right>^2 + 12\left<x^2\right>\left<x\right>^2-6\left<x\right>^4
    .\end{aligned}\]
    Possiamo dimostrare che $\left<\left<x^4\right>\right> = 0$ valutando $\frac{d}{dx}\left<x^3\right>$:
    \[
        \int_{-\infty}^{\infty} \frac{\text{d} }{\text{d} x} \left\{x^3 \exp\left(\frac{-x^2}{2\sigma^2}\right)\right\} dx = 0 
    .\] 
    Questa si azzera perché la Gaussiana si annulla in $x = \pm \infty$, la precedente equazione può essere riscritta tramite le regole della derivata composta:
    \[
        \int\left(3x^2\exp\left(-\frac{x^2}{2\sigma^2}\right) - 
		\frac{x^4}{\sigma^2}\exp\left(- \frac{x^2}{2\sigma^2}\right) \right)dx = 0 
    .\] 
    \begin{equation}
 	\left<x^4\right> = 3\sigma^2\left<x^2\right> = 3 \left(\sigma^2\right)^2
	\label{eq:cum4_gauss}
    \end{equation}
    Inserendo nella equazione per il cumulante quarto si annullano tutti i termini (per semplicità abbiamo preso una gaussiana a media nulla $\left<x\right>=0$ ) .
\end{exmp}
\noindent
In generale questa cosa non funziona, non è possibile esprimere i cumulanti in funzione di altri cumulanti di ordine inferiore per ogni distribuzione.

\subsection{Teorema del limite centrale}%
\label{sub:Teorema del limite centrale}
\begin{redbox}{Teorema del limite centrale}
   La somma di variabili stocastiche aventi media e varianza definita tende ad una Gaussiana. 
\end{redbox}
\noindent
Sia $\left\{x_i\right\}$ una variabile random con distribuzione di probabilità $P_i(x_i)$, il teorema richiede che i primi due momenti siano definiti:
\[\begin{aligned}
    & \left<x_i\right> = 0; &&
    & \text{var}\left\{x_i^2\right\} = b_i^2
.\end{aligned}\]
Si definiscono le seguenti quantità:
\[\begin{aligned}
    & s_n = \sum_{i=1}^{n} x_i;
    &&
    &\sigma_n^2 = \sum_{i=1}^{n} b_i^2
.\end{aligned}\]
Se le code della $s_n$ si annullano in modo "rapido" secondo la seguente equazione:
\[
    \lim_{n \to \infty} 
    \left[\frac{1}{\sigma_n^2} \sum_{i=1}^{n} \int\limits_{\left|x\right|>t\sigma_n}^{} dx x^2_i P_i(x)  \right] 
    = 0 \qquad \forall t>0
.\] 
Se ne conclude che $s_n/\sigma_n$ tende ad una gaussiana di media nulla e varianza unitaria.
\[
    \tilde{s}_n = s_n /\sigma_n \to G \qquad \mbox{Per} \ n \to \infty
.\] 
\begin{exmp}[Distribuzione che non tende ad una Gaussiana]
    Una somma di variabili stocastiche distribuite secondo una Lorenziana non tende ad una Gaussiana perché il suo momento secondo diverge.
\end{exmp}
\noindent
\subsubsection{Teorema di Chebyshev}%
\label{subsub:Teorema di Chebyshev}
Si cerca di quantificare quanto velocemente una distribuzione tenda alla Gaussiana.\\
Definiamo la funzione:
\[
    F_n(t) \equiv \int_{-\infty}^{t} \tilde{P}(\tilde{s}_n)d\tilde{s}_n;  \qquad \phi (t) = \lim_{n \to \infty} F_n(t) 
.\] 
Dove $\tilde{P}$ è la distribuzione dei $\tilde{s}_n$, che tiene di conto che ad ogni step $n$ cambia la normalizzazione necessaria per essere una probabilità.
\begin{redbox}{Teorema di Chebyshev}
    \[
	F_n-\phi (t) \sim \frac{e^{-t^2 /2}}{\sqrt{2\pi}}\left[\frac{Q_1(t)}{n^{1 /2}} + \frac{Q_2(t)}{n} + \ldots\right]
    .\] 
    In cui i $Q_i$ sono i polinomi di Chebyshev-Hermite, legati ai momenti di $\left\{x_i\right\}$.
\end{redbox}
\noindent
Prendiamo ad esempio $Q_1(t)$:
\[
    Q_1(t) \propto \frac{\left<\left(x-\left<x\right>\right)^3\right>}{\sigma^3}
.\] 
la quantità a destra è legata al momento terzo di $\left\{x_i\right\}$, di conseguenza è nulla nel caso gaussiano (e lo sono anche tutte le restanti $Q_i$).\\
In conclusione le distribuzioni tendono ad una Gaussiana nelle ipotesi del teorema del limite centrale come $1 / \sqrt{n} $.

\subsection{Momenti fattoriali}%
\label{sub:Momenti fattoriali}
I momenti fattoriali di una distribuzione $f$ sono definiti nel seguente modo:
\begin{redbox}{Momenti fattoriali}
    \[
	\left<n^r\right>_f = \left<n\cdot (n-1) \cdot \ldots \cdot (n-r+1) \right>
    .\] 
\end{redbox}
\noindent
\subsubsection{Momenti fattoriali della distribuzione di Poisson}%
\label{subsub:Momenti fattoriali della distribuzione di Poisson}
Prendiamo la distribuzione:
\[
	P(n) = e^{-\lambda } \frac{\lambda^n}{n!\,}
.\] 
\begin{exmp}[Poisson con $r=2$.]
    Sia $f$ Poissoniana:
   \[\begin{aligned}
       \left<x^2\right>_f=\left<x\left(x-1\right)\right>_f =& \sum_{x = 0}^{\infty} x\left(x-1\right)e^{-\lambda} \frac{\lambda^x}{x!\,} = \\
       =& \lambda^2 \sum_{x = 0}^{\infty}  e^{-\lambda} \frac{\lambda^{x-2}}{\left(x-2\right)!} = \lambda^2
   .\end{aligned}\]
   L'ultimo passaggio lo si ottiene dal fatto che l'indice della sommatoria può essere traslato e che tale sommatoria si estende ad $\infty$, quindi la somma corrisponde a $\left<f\right> \equiv 1$.
\end{exmp}
\noindent
Iterando questa procedura si ottiene:
\begin{bluebox}{Momenti fattoriali per distribuzione di Poisson}
 \[
    \left<x^r\right>_f = \lambda^r
.\]    
\end{bluebox}
\noindent

\subsection{Funzione generatrice generalizzata.}%
\label{subsub:Funzione generatrice generalizzata.}

\begin{redbox}{Funzione generatrice}
    \[
	G(s) = \sum_{n=0}^{\infty} s^n P(n) = \left<s^n\right>
    .\] 
\end{redbox}
\noindent
Possiamo ottenere la $G(s)$ a partire dalla funzione caratteristica:
\[
    G(s) = \phi (-i\ln s) 
.\] 
Grazie a questa possiamo esprimere i momenti fattoriali nel seguente modo:
\[
    \left<x^n\right>_f = \left[\frac{\partial^n}{\partial s^n} G(s)\right]_{s=1}
.\] 
\begin{bluebox}{Funzione generatrice dei cumulanti fattoriali}
    \[
	g(s) \equiv \ln (G(s)) = \sum_{r=1}^{\infty} \left<\left<x^r\right>\right>_f \frac{\left(s-1\right)^r}{r!}
    .\] 
\end{bluebox}
\noindent
I cumulanti fattoriali, come quelli visti nella sezione precedente, sono tabulati per ciascuna distribuzione.
\begin{exmp}[Funzione generatrice per Poissoniana]
    \[\begin{aligned}
	G(s) =& \sum_{n=1}^{\infty} s^n e^{-\lambda} \frac{\lambda^n}{n!\,} =\\
	=& e^{-\lambda}\sum_{n=1}^{\infty} \frac{\left(s\lambda\right)^n}{n!\,} =  e^{\lambda (s-1) }
     .\end{aligned}\]
     Che è lo stesso risultato ottenuto nella \ref{subsec:fgen_met}.
     Per Poisson si ha quindi che:
     \[
         \left<\left<x^r\right>\right>_f = 0 \quad \forall r \ge  2
     .\] 
\end{exmp}
\noindent
\clearpage
